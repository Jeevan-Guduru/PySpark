{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd \n",
    "import numpy\n",
    "import matplotlib.pyplot as plt \n",
    "from pyspark.sql import SparkSession\n",
    "# create sparksession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pysparkexample\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.25 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "843 ms ± 467 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df1=spark.read.csv('Vermont_Vendor_Payments.csv',header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 10.30 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "19.1 s ± 18.3 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df_pandas=pd.read_csv('Vermont_Vendor_Payments.csv',low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#For this analysis I will read in the data using the inferSchema option and cast the Amount column to a double."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('Vermont_Vendor_Payments.csv', header='true', inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Amount\", df[\"Amount\"].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Quarter Ending',\n",
       " 'Department',\n",
       " 'UnitNo',\n",
       " 'Vendor Number',\n",
       " 'Vendor',\n",
       " 'City',\n",
       " 'State',\n",
       " 'DeptID Description',\n",
       " 'DeptID',\n",
       " 'Amount',\n",
       " 'Account',\n",
       " 'AcctNo',\n",
       " 'Fund Description',\n",
       " 'Fund']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Spark Methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Quarter Ending',\n",
       " 'Department',\n",
       " 'UnitNo',\n",
       " 'Vendor Number',\n",
       " 'Vendor',\n",
       " 'City',\n",
       " 'State',\n",
       " 'DeptID Description',\n",
       " 'DeptID',\n",
       " 'Amount',\n",
       " 'Account',\n",
       " 'AcctNo',\n",
       " 'Fund Description',\n",
       " 'Fund']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of rows: 1680170 \n",
      " The total number of column is: 14\n"
     ]
    }
   ],
   "source": [
    "print('The total number of rows:',df.count(),'\\n The total number of column is:',len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show() - prints the first 20 rows of the dataframe by default. I chose to only print 5 in this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+------+-------------+--------------------+----------+-----+--------------------+----------+--------+--------------------+------+--------------------+-----+\n",
      "|Quarter Ending|          Department|UnitNo|Vendor Number|              Vendor|      City|State|  DeptID Description|    DeptID|  Amount|             Account|AcctNo|    Fund Description| Fund|\n",
      "+--------------+--------------------+------+-------------+--------------------+----------+-----+--------------------+----------+--------+--------------------+------+--------------------+-----+\n",
      "|    09/30/2009|Environmental Con...|  6140|   0000276016|1st Run Computer ...|      null|   NY|     WQD - Waterbury|6140040206|   930.0|Rep&Maint-Info Te...|513000|Environmental Per...|21295|\n",
      "|    09/30/2009|Environmental Con...|  6140|   0000276016|1st Run Computer ...|      null|   NY|Water Supply Divi...|6140040406|   930.0|Rep&Maint-Info Te...|513000|Environmental Per...|21295|\n",
      "|    09/30/2009|Vermont Veterans'...|  3300|   0000284121| 210 Innovations LLC|      null|   CT|         MAINTENANCE|3300010300|    24.0|Freight & Express...|517300|    Vermont Medicaid|21782|\n",
      "|    09/30/2009|Vermont Veterans'...|  3300|   0000284121| 210 Innovations LLC|      null|   CT|         MAINTENANCE|3300010300|   420.0|Building Maintena...|520200|    Vermont Medicaid|21782|\n",
      "|    09/30/2009|         Corrections|  3480|   0000207719|21st Century Cell...|      null|   PA|     Brattleboro P&P|3480004630|   270.8|Telecom-Wireless ...|516659|        General Fund|10000|\n",
      "|    09/30/2009|         Corrections|  3480|   0000207719|21st Century Cell...|      null|   PA|     Springfield P&P|3480004730|    35.0|Telecom-Wireless ...|516659|        General Fund|10000|\n",
      "|    09/30/2009|       Public Safety|  2140|   0000010258|               3M Co|      null|   PA|DPS-AD-SS-Off of ...|2140011220|   971.4|Other General Sup...|520500|        General Fund|10000|\n",
      "|    09/30/2009|Agriculture, Food...|  2200|   0000010258|               3M Co|      null|   TX|Ag Development Di...|2200030000|   60.59|Medical and Lab S...|521810|        General Fund|10000|\n",
      "|    09/30/2009|Agriculture, Food...|  2200|   0000010258|               3M Co|      null|   TX|  Laboratory Section|2200043000|  541.62|Medical and Lab S...|521810|        General Fund|10000|\n",
      "|    09/30/2009|              Health|  3420|   0000010258|               3M Co|      null|   PA| HS Lab-Microbiology|3420021209|  283.98|Medical and Lab S...|521810|Federal Revenue Fund|22005|\n",
      "|    09/30/2009|              Health|  3420|   0000010258|               3M Co|      null|   TX| HS Lab-Microbiology|3420021209|  505.35|Medical and Lab S...|521810|        General Fund|10000|\n",
      "|    09/30/2009|VT Offender Work ...|  3675|   0000010258|               3M Co|      null|   PA|VOWP-Resale-Sheet...|3675091010| 23473.1|Road Supplies and...|521600|Correctional Indu...|59100|\n",
      "|    09/30/2009|VT Offender Work ...|  3675|   0000010258|               3M Co|      null|   PA|VOWP - Sign Shop ...|3675091011|  -495.0|Road Supplies and...|521600|Correctional Indu...|59100|\n",
      "|    09/30/2009|Agency of Transpo...|  8100|   0000010258|               3M Co|      null|   TX|         Maintenance|8100002000|  1735.2|Road Supplies and...|521600|Transp Fund - Non...|20105|\n",
      "|    09/30/2009|Forest, Parks & R...|  6130|   0000277548|4 Generations Mas...|      null|   VT|Rehab Aging St Pa...|6130990801|  6150.0|Other Contr and 3...|507600|Natural Resources...|31500|\n",
      "|    09/30/2009|Forest, Parks & R...|  6130|   0000277548|4 Generations Mas...|      null|   VT|Rehab Aging St Pa...|6130990801|  2400.0|Other Repair & Ma...|513200|Natural Resources...|31500|\n",
      "|    09/30/2009|Forest, Parks & R...|  6130|   0000277548|4 Generations Mas...|      null|   VT|Rehab Aging St Pa...|6130990801|  3500.0|Repairs&Maint-Pro...|513210|Natural Resources...|31500|\n",
      "|    06/30/2017|Defender General'...|  2110|   0000043657|Buildings & Gener...|Montpelier|   VT|        AC Caledonia|2110011500|   14.94|Postage-BGS Posta...|517205|        General Fund|10000|\n",
      "|    09/30/2009|Agency of Transpo...|  8100|   0000016981|        4imprint Inc|      null|   WI|         Maintenance|8100002000| 1352.61|    Cloth & Clothing|520520|Transp Fund - Non...|20105|\n",
      "|    09/30/2009|Children and Fami...|  3440|   0000268665|5 Star Childcare ...|      null|   VT|DCFS - Child Deve...|3440030000|23989.49|Child Care Subsid...|603500|Federal Revenue Fund|22005|\n",
      "+--------------+--------------------+------+-------------+--------------------+----------+-----+--------------------+----------+--------+--------------------+------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.head of DataFrame[Quarter Ending: string, Department: string, UnitNo: int, Vendor Number: string, Vendor: string, City: string, State: string, DeptID Description: string, DeptID: string, Amount: double, Account: string, AcctNo: string, Fund Description: string, Fund: string]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in pandas, we can call the describe method to get basic numerical summaries of the data. We need to use the show method to print it to the notebook.This does not print very nicely in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------------------+------------------+------------------+--------------------+--------+------------------+------------------+--------------------+--------------------+--------------------+-----------------+--------------------+--------------------+\n",
      "|summary|Quarter Ending|          Department|            UnitNo|     Vendor Number|              Vendor|    City|             State|DeptID Description|              DeptID|              Amount|             Account|           AcctNo|    Fund Description|                Fund|\n",
      "+-------+--------------+--------------------+------------------+------------------+--------------------+--------+------------------+------------------+--------------------+--------------------+--------------------+-----------------+--------------------+--------------------+\n",
      "|  count|       1680169|             1680169|           1680169|           1680169|             1680169|  937846|           1680121|           1679632|             1680169|             1679823|             1680169|          1680169|             1680167|             1680168|\n",
      "|   mean|          null|                null|4065.1289572656083|105263.82263244389|                null|     0.0|1.6129032258064515|              null|4.0664174870284295E9|   185564.9029836577| 7.077523825759754E8|532103.2618668326|  517531.46396396396|  25948.171205401122|\n",
      "| stddev|          null|                null| 2330.606553508123|121452.79234164105|                null|     0.0| 10.93859566742073|              null|2.3302904685984216E9|1.4200185374028629E7| 5.665974429098641E8|30104.87438293861|   4467.543753675303|  19236.411235815693|\n",
      "|    min|    03/31/2010|AOT Proprietary F...|              1100|        0000000002|\"Jewett,Martin A ...|       0|                 0|         \"\"\"Admin.|              CCV\"\"\"|         -2880183.34|             -294.00|          -294.00|              507200|               10000|\n",
      "|    max|    12/31/2019|  Women's Commission|              9150|            SINGLE|           xAd, Inc.|w Berlin|                ZZ|     Youth at Risk|                 Seg|           6.10001E9|Youth Development...|      Water/Sewer|Youth Substance A...|Facilities Operat...|\n",
      "+-------+--------------+--------------------+------------------+------------------+--------------------+--------+------------------+------------------+--------------------+--------------------+--------------------+-----------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying the data:\n",
    "One of the strengths of Spark is that it can be queried with each language’s respective Spark library or with Spark SQL. following will demonstrate a few queries using both the pythonic and SQL options.\n",
    "The following code registers temporary table and selects a few columns using SQL syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+------+-----+\n",
      "|Quarter Ending|          Department|Amount|State|\n",
      "+--------------+--------------------+------+-----+\n",
      "|    09/30/2009|Environmental Con...| 930.0|   NY|\n",
      "|    09/30/2009|Environmental Con...| 930.0|   NY|\n",
      "|    09/30/2009|Vermont Veterans'...|  24.0|   CT|\n",
      "|    09/30/2009|Vermont Veterans'...| 420.0|   CT|\n",
      "|    09/30/2009|         Corrections| 270.8|   PA|\n",
      "|    09/30/2009|         Corrections|  35.0|   PA|\n",
      "|    09/30/2009|       Public Safety| 971.4|   PA|\n",
      "|    09/30/2009|Agriculture, Food...| 60.59|   TX|\n",
      "|    09/30/2009|Agriculture, Food...|541.62|   TX|\n",
      "|    09/30/2009|              Health|283.98|   PA|\n",
      "+--------------+--------------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a temporary table query with SQL\n",
    "df.createOrReplaceTempView('VermontVendor')\n",
    "spark.sql(\n",
    "'''\n",
    "SELECT `Quarter Ending`, Department, Amount, State FROM VermontVendor\n",
    "LIMIT 10\n",
    "'''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+------+-----+\n",
      "|Quarter Ending|          Department|Amount|State|\n",
      "+--------------+--------------------+------+-----+\n",
      "|    09/30/2009|Environmental Con...| 930.0|   NY|\n",
      "|    09/30/2009|Environmental Con...| 930.0|   NY|\n",
      "|    09/30/2009|Vermont Veterans'...|  24.0|   CT|\n",
      "|    09/30/2009|Vermont Veterans'...| 420.0|   CT|\n",
      "|    09/30/2009|         Corrections| 270.8|   PA|\n",
      "|    09/30/2009|         Corrections|  35.0|   PA|\n",
      "|    09/30/2009|       Public Safety| 971.4|   PA|\n",
      "|    09/30/2009|Agriculture, Food...| 60.59|   TX|\n",
      "|    09/30/2009|Agriculture, Food...|541.62|   TX|\n",
      "|    09/30/2009|              Health|283.98|   PA|\n",
      "+--------------+--------------------+------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This code performs pretty much the same operation using pythonic syntax:\n",
    "df.select('Quarter Ending', 'Department', 'Amount', 'State').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#One thing to note is that the pythonic solution is significantly less code. I like SQL and it’s syntax, so I prefer the SQL interface over the pythonic one.\n",
    "#I can filter the columns selected in my query using the SQL WHERE clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the code for filtering the columns selected in my query using the SQL WHERE clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-------+-----+\n",
      "|Quarter Ending|Department| Amount|State|\n",
      "+--------------+----------+-------+-----+\n",
      "|    09/30/2009| Education|9423.36|   VT|\n",
      "|    09/30/2009| Education| 110.03|   IL|\n",
      "|    09/30/2009| Education| 332.58|   IL|\n",
      "|    09/30/2009| Education|  60.08|   IL|\n",
      "|    09/30/2009| Education| 284.83|   IL|\n",
      "|    09/30/2009| Education| 377.15|   IL|\n",
      "|    09/30/2009| Education| 114.74|   IL|\n",
      "|    09/30/2009| Education| 129.72|   IL|\n",
      "|    09/30/2009| Education| 114.54|   IL|\n",
      "|    09/30/2009| Education|  375.6|   IL|\n",
      "+--------------+----------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "'''\n",
    "\n",
    "SELECT `Quarter Ending`, Department, Amount, State FROM VermontVendor \n",
    "WHERE Department = 'Education'\n",
    "LIMIT 10\n",
    "\n",
    "'''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar result can be achieved with the .filter() method in the python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-------+-----+\n",
      "|Quarter Ending|Department| Amount|State|\n",
      "+--------------+----------+-------+-----+\n",
      "|    09/30/2009| Education|9423.36|   VT|\n",
      "|    09/30/2009| Education| 110.03|   IL|\n",
      "|    09/30/2009| Education| 332.58|   IL|\n",
      "|    09/30/2009| Education|  60.08|   IL|\n",
      "|    09/30/2009| Education| 284.83|   IL|\n",
      "|    09/30/2009| Education| 377.15|   IL|\n",
      "|    09/30/2009| Education| 114.74|   IL|\n",
      "|    09/30/2009| Education| 129.72|   IL|\n",
      "|    09/30/2009| Education| 114.54|   IL|\n",
      "|    09/30/2009| Education|  375.6|   IL|\n",
      "+--------------+----------+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Quarter Ending', 'Department', 'Amount', 'State').filter(df['Department'] == 'Education').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'some-value'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.some.config.option')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#above is an overview of using SQL in spark. Following will have all the important contents and function of PySpark\n",
    "#creating Spark Dataframe from an RDD, a list or a pandas.DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='john', _2='40')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)[source]\n",
    "#from list\n",
    "l=[('john','40')]\n",
    "spark.createDataFrame(l).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='john', age='40')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#guving schema\n",
    "spark.createDataFrame(l,['name','age']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(age='40', name='John')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dictionary\n",
    "d=[{'name':'John','age':'40'}]\n",
    "spark.createDataFrame(d).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-406265e098c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#dataframe from rdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "#dataframe from rdd\n",
    "rdd = sc.parallelize(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: rdds can be created only with sparkContext but not with sparkSession\n",
    "#In Spark 2+, Spark Context is available via Spark Session, so all you need to do is:\n",
    "#spark.sparkContext().parallelize(l)\n",
    "rdd=spark.sparkContext.parallelize(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='john', _2='40')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataframe from rdd now:\n",
    "spark.createDataFrame(rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='john', age='40')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding schema parameter-list of columns\n",
    "spark.createDataFrame(rdd,['name','age']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='john', age='40')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating schema from Row method of pyspark.sql\n",
    "#observe the P,p in persons carefully - map funtion is used to match the schmea to each row in the rdd created above\n",
    "Person=Row('name','age')\n",
    "person=rdd.map(lambda r: Person(*r))\n",
    "df2=spark.createDataFrame(person)\n",
    "df2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='john', age='40')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating spark dataframe from pandas dataframe\n",
    "spark.createDataFrame(df2.toPandas()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://a3613f5d1852:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Pysparkexample</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe369b2ef40>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting current active spark session\n",
    "spark.getActiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrameReader' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-65c4b8d324a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'DataFrameReader' object is not callable"
     ]
    }
   ],
   "source": [
    "spark.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://a3613f5d1852:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Pysparkexample</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Pysparkexample>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#returns the underlying psarkContext\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-54-cf46469d4728>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-54-cf46469d4728>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    from pyspark.sql.SQLContext(none,sparkSession=spark)\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Data frame created from SQLContext similar to SparkSession like  above -it is actually replaced by SparkSession in spark 2.0\n",
    "#class pyspark.sql.SQLContext(sparkContext, sparkSession=None, jsqlContext=None)[source]\n",
    "from pyspark.sql.SQLContext(none,sparkSession=spark)\n",
    "sqlContext.createDataFrame(l).collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-56-978da5640423>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-56-978da5640423>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    import pyspark.sql.HiveContext(sparkContext=spark)\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.HiveContext(sparkContext=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
